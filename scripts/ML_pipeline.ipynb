{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this notebook we attempt to build xgboost and linear regression models using the sklearn pipeline framework\n",
    "# At this point nothing special yet. We simply create a class that selects columns, a class that can transform\n",
    "# numerical columns, and a class that z-score normalises columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standard libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "# Transformer objects:\n",
    "from sklearn.base import TransformerMixin, BaseEstimator, clone\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.svm import LinearSVC\n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# More pipeline stuff\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/maxthone/Documents/Personal/Python_Projects/kaggle_housing'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#os.chdir(os.path.dirname(os.getcwd()))\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('data/input_data/train.csv')\n",
    "df_test = pd.read_csv('data/input_data/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove some outliers (shouldn't this be part of the pipeline?)\n",
    "df_train = df_train.loc[df_train['GrLivArea'] < 4000,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a train/test split.\n",
    "X_train,X_test,y_train,y_test = train_test_split(\n",
    "    df_train.drop('SalePrice',axis = 1),\n",
    "    df_train['SalePrice'],\n",
    "    test_size = 0.3,\n",
    "    random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SelectColumnsTransformer(BaseEstimator,TransformerMixin):\n",
    "    \"\"\"A DataFrame transformer that provides column selection\"\"\"\n",
    "    \n",
    "    def __init__(self,columns = []):\n",
    "        self.columns = columns\n",
    "    \n",
    "    def transform(self,X, **transform_params):\n",
    "        trans = X.loc[:,self.columns].copy()\n",
    "        return trans\n",
    "    \n",
    "    def fit(self, X,y = None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class base_transformer(BaseEstimator,TransformerMixin):\n",
    "    '''basic function to apply transformations, and where no fit is needed'''\n",
    "    def __init__(self,func):\n",
    "        self.func = func\n",
    "    \n",
    "    def transform(self,X,**transform_params):\n",
    "        trans = pd.DataFrame(X).apply(self.func).copy()\n",
    "        return trans\n",
    "    \n",
    "    def fit(self, X,y = None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class My_Scaler(BaseEstimator,TransformerMixin):\n",
    "    '''DataFrame transformer that applies normalisation scaling to \n",
    "    numerical columns. \n",
    "    '''\n",
    "    def transform(self,X,**transform_params):\n",
    "        trans = X.apply(lambda x: (x - self.mu_series)/self.sd_series, axis=1).copy()\n",
    "        return trans\n",
    "        \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        self.mu_series = X.apply(lambda x: np.mean(x))\n",
    "        self.sd_series = X.apply(lambda x: np.std(x))\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having defined our  transfomer classes, we can now start building pipelines.\n",
    "# We start with a basic linear regression and xgboost pipelines. The only thing they do\n",
    "# is take two numerical variables, take the logarithm of them, and then normalise, before they enter\n",
    "# the ML algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_reg_pipeline = Pipeline([\n",
    "    ('selector',SelectColumnsTransformer(columns = ['GrLivArea','OverallQual'])),\n",
    "    ('log_transform',base_transformer(np.log)),\n",
    "    ('scaler',My_Scaler()),\n",
    "    ('lin_reg',LinearRegression())])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxthone/.virtualenv/ml_venv/lib/python3.6/site-packages/sklearn/linear_model/base.py:509: RuntimeWarning: internal gelsd driver lwork query error, required iwork dimension not returned. This is likely the result of LAPACK bug 0038, fixed in LAPACK 3.2.2 (released July 21, 2010). Falling back to 'gelss' driver.\n",
      "  linalg.lstsq(X, y)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6913712339273694"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipe_test = lin_reg_pipeline.fit(X_train,y_train)\n",
    "pipe_test.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg_boost_pipeline = Pipeline([\n",
    "    ('selector',SelectColumnsTransformer(columns = ['GrLivArea','OverallQual'])),\n",
    "    ('log_transform',base_transformer(np.log)),\n",
    "    ('scaler',My_Scaler()),\n",
    "    ('xgb_reg',xgb.XGBRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.803562830166198"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_test = xg_boost_pipeline.fit(X_train,y_train)\n",
    "xgb_test.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Already with two variables and not doing any tuning we see that the XGBoost algorithm greatly outperforms the \n",
    "# linear regression algorithm, due to it being able to capture more non-linear relationships between the independent\n",
    "# and dependent variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next we are going to add in a categorical variable. \n",
    "# A logical one to consider is Neighborhoods, since big determinant of the price of a house. \n",
    "# What we are going to do: At the end we need a one hot encoder: One hot encoder only takes numpy arrays\n",
    "# with the shape of (length(vector),1), so we need an array reshaper to do that for us\n",
    "# We also use a shelf Label Encoder that transforms the category string labels to numerical labels\n",
    "# For some reason this Label Encoder does not work directly in a pipeline, so we need to create a wrapper around it\n",
    "# so that it does.\n",
    "\n",
    "# Finally, Neighborhoods has a pretty high cardinality (25 different neighborhoods). So we can apply a Kmeans algorithm\n",
    "# that groups the neighborhoods up by sale price data in the training set.\n",
    "\n",
    "# In summary: from sklearn we use LabelEncoder, OneHotEncoder and KMeans\n",
    "# We create some transformer classes so that we can implement these classes in our pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class array_reshaper(BaseEstimator, TransformerMixin):\n",
    "    '''reshape transformer to reshape numpy arrays '''\n",
    "    \n",
    "    def transform(self, X, ** transform_params):\n",
    "        X = np.array(X)\n",
    "        trans = X.reshape([X.shape[0],1])\n",
    "        return trans\n",
    "    \n",
    "    def fit(self, X, y = None, **fit_params):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelEncoderWrapper(BaseEstimator, TransformerMixin):\n",
    "    '''wrapper around label encoder, for the reason that label encoder \n",
    "    fit_transform does not take in X\n",
    "    '''\n",
    "    def fit(self, X, y = None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self,X,y=None):\n",
    "        enc = LabelEncoder()\n",
    "        return enc.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class cluster_builder(BaseEstimator,TransformerMixin):\n",
    "    '''\n",
    "    transformer that returns clusters for a given categorical variable. Useful for\n",
    "    categorical variables with high cardinality. Clusters are based on y_train.\n",
    "    '''\n",
    "    def __init__(self,n_clusters):\n",
    "        self.clusters = n_clusters\n",
    "        self.model = KMeans(self.clusters)\n",
    "    \n",
    "    def transform(self,X,**transform_params):\n",
    "        trans = (X.reset_index()\n",
    "                 .merge(self.df,how = 'left', on = [X.columns.values[0]]).set_index('index'))\n",
    "        return trans['cluster_grp']\n",
    "    \n",
    "    def fit(self,X,y,**fit_params):\n",
    "        init_df = pd.concat([X,y], axis = 1)\n",
    "        self.df = init_df.groupby(X.columns.values[0]).agg({y.name:'mean'}).reset_index()\n",
    "        kmeans_input = np.array(self.df.iloc[:,-1])\n",
    "        kmeans_input = kmeans_input.reshape([len(kmeans_input),1])\n",
    "        self.df['cluster_grp'] = self.model.fit_predict(kmeans_input)\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Having created our new classes, we now can create our pipelines:\n",
    "# A pipeline for neighborhood where we don't cluster (NB_one_hot_pipeline)\n",
    "# A pipeline for neighborhood where we DO cluster (NB_cluster_pipeline)\n",
    "# A pipeline for num variables where we apply log and normalisation transforms (num_pipeline)\n",
    "# Finally we combine these pipelines, using featureunion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_one_hot_pipeline = Pipeline([\n",
    "    ('selector', SelectColumnsTransformer(columns = ['Neighborhood'])),\n",
    "    ('label_transform',LabelEncoderWrapper()),\n",
    "    ('array_reshape',array_reshaper()),\n",
    "    ('one_hot',OneHotEncoder(sparse = False)) \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "NB_cluster_pipeline = Pipeline(\n",
    "    [('selector',SelectColumnsTransformer(columns=['Neighborhood'])),\n",
    "     ('build_clusters',cluster_builder(6)),\n",
    "     ('array_reshape',array_reshaper()),\n",
    "     ('one_hot',OneHotEncoder(sparse=False))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_pipeline = Pipeline([\n",
    "    ('selector',SelectColumnsTransformer(columns = ['GrLivArea','OverallQual'])),\n",
    "    ('log_transform',base_transformer(np.log)),\n",
    "    ('scaler',My_Scaler())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pipeline_1 = Pipeline([\n",
    "    ('union' , FeatureUnion(\n",
    "    transformer_list = [\n",
    "        ('Neighborhood_one_hot',NB_one_hot_pipeline),\n",
    "        ('num_pipeline', num_pipeline)])),\n",
    "    ('xgb_reg',xgb.XGBRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "Final_pipeline_2 = Pipeline([\n",
    "    ('union' , FeatureUnion(\n",
    "    transformer_list = [\n",
    "        ('Neighborhood_cluster',NB_cluster_pipeline),\n",
    "        ('num_pipeline', num_pipeline)])),\n",
    "    ('xgb_reg',xgb.XGBRegressor())\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/maxthone/.virtualenv/ml_venv/lib/python3.6/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "/Users/maxthone/.virtualenv/ml_venv/lib/python3.6/site-packages/sklearn/preprocessing/label.py:111: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8879328124580523"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_test_1 = Final_pipeline_1.fit(X_train,y_train)\n",
    "xgb_test_1.score(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.813659246061051"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_test_2 = Final_pipeline_2.fit(X_train,y_train)\n",
    "xgb_test_2.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using neighborhoods without clusters actually leads to a reduction in \n",
    "# in accuracy on test data, compard to not using \n",
    "# Neigborhoods at all. This is due to overfitting: There are too many variables\n",
    "# for the algorithm to fit to with only 1019 data points.\n",
    "\n",
    "# We see that clustering neighborhoods actually works: Accuracy on test data has improved by \n",
    "# 1%.\n",
    "\n",
    "# However bear in mind that we need to have a separate data set for scoring again: (X_val, y_val), \n",
    "# because otherwise you tune the hyperparameters (k-means number of clusters in this example, or XGboost params) \n",
    "# too much to your test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## WORK IN PROGRESS ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Work in progress: transformer that replaces nulls. Mostly for numerical I think, but perhaps can make it more\n",
    "# general for categorical columns as well. \n",
    "class null_replacer(BaseEstimator,TransformerMixin):\n",
    "    # Removes all columns with a certain ratio of nulls:\n",
    "    # For numeric columns, we want to replace the numeric value with some numeric_function\n",
    "    def __init__(self,impute_func):\n",
    "    \n",
    "    def transform(self,X,**transform_params):\n",
    "    \n",
    "    def fit(self,X,y = None, **fit_params):\n",
    "        self.num_replacer = X.apply(self.func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# useful wrapper for around ML algorithms that you want to use within the pipeline (instead of as an end point)\n",
    "# E.G. when you want to reduce the cardinality of categorical variables using KMeans\n",
    "class ModelTransformer(BaseEstimator, TransformerMixin):\n",
    "    '''\n",
    "    wrapper for around Kmeans, to make it return predictions.\n",
    "    default behaviour of fit_transform is an array of arrays, which is not\n",
    "    useful for us. \n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def transform(self, X, **transform_params):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def fit(self, *args, **kwargs):\n",
    "        self.model.fit(*args, **kwargs)\n",
    "        return self"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_kernel",
   "language": "python",
   "name": "ml_kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
